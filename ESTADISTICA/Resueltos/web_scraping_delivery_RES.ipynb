{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. From HTML\n",
    "\n",
    "*Using only beautiful soap*\n",
    "\n",
    "Save in a dataframe the next information using web scraping. Each row of the dataframe must have in different columns:\n",
    "\n",
    "- The name of the title\n",
    "- The id of the div where is the value scraped. If there is not id, then the value is must be numpy.nan\n",
    "- The name of the tag where is the value scraped.\n",
    "- The next scraped values in different rows: \n",
    "    - The value: \"Este es el segundo párrafo\"  --> Row 1\n",
    "    - The url https://pagina1.xyz/ --> Row 2\n",
    "    - The url https://pagina4.xyz/ --> Row 3\n",
    "    - The url https://pagina5.xyz/ --> Row 4\n",
    "    - The value \"links footer-links\" --> Row 5\n",
    "    - The value \"Este párrafo está en el footer\" --> Row 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = \"\"\"<html lang=\"es\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>Página de prueba</title>\n",
    "</head>\n",
    "<body>\n",
    "<div id=\"main\" class=\"full-width\">\n",
    "    <h1>El título de la página</h1>\n",
    "    <p>Este es el primer párrafo</p>\n",
    "    <p>Este es el segundo párrafo</p>\n",
    "    <div id=\"innerDiv\">\n",
    "        <div class=\"links\">\n",
    "            <a href=\"https://pagina1.xyz/\">Enlace 1</a>\n",
    "            <a href=\"https://pagina2.xyz/\">Enlace 2</a>\n",
    "        </div>\n",
    "        <div class=\"right\">\n",
    "            <div class=\"links\">\n",
    "                <a href=\"https://pagina3.xyz/\">Enlace 3</a>\n",
    "                <a href=\"https://pagina4.xyz/\">Enlace 4</a>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "    <div id=\"footer\">\n",
    "        <!-- El footer -->\n",
    "        <p>Este párrafo está en el footer</p>\n",
    "        <div class=\"links footer-links\">\n",
    "            <a href=\"https://pagina5.xyz/\">Enlace 5</a>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "</body>\n",
    "</html>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"index.html\", \"w\") as f:\n",
    "    f.write(html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un diccionario donde guardaremos los valores del futuro dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_dict = {\"Title\": [], \"Id\": [], \"Tag\": [], \"Value\": []}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puesto que salvo para la columna `Title`, el resto de procesos para acceder a los datos requeridos es independiente y por tanto, no parametrizable, resolveremos el ejercicio, fila a fila."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fila 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title ['Página de prueba']\n",
      "id ['main']\n",
      "tag ['p']\n",
      "value ['Este es el primer párrafo']\n"
     ]
    }
   ],
   "source": [
    "# Cómo encontramos el título es común a todas las filas. Lo obtenemos y metemos en la lista contenida \n",
    "# como value del diccionario vacío creado anteriormente\n",
    "title_content = soup.find('title')\n",
    "title_content_str = title_content.string\n",
    "wb_dict[\"Title\"].append(title_content_str)\n",
    "print(\"title\", wb_dict[\"Title\"])\n",
    "\n",
    "# Obtenemos el 'Id', en este caso 'main'\n",
    "result = []\n",
    "for tag in soup.findAll(True,{'id':True}) :\n",
    "    result.append(tag['id'])\n",
    "\n",
    "wb_dict[\"Id\"].append(result[0])\n",
    "print(\"id\", wb_dict[\"Id\"])\n",
    "\n",
    "# Para obtener el tag accedemos a `p` y seleccionamos el nombre de este que es lo que \n",
    "# añadiremos al diccionario \n",
    "soup.find(\"p\").name\n",
    "wb_dict[\"Tag\"].append(soup.find(\"p\").name)\n",
    "\n",
    "print(\"tag\", wb_dict[\"Tag\"])\n",
    "\n",
    "# Obtenemos el contenido en el tag `p` y lo guardamos en `wb_dict`\n",
    "soup.find(\"p\").contents\n",
    "wb_dict[\"Value\"].append(soup.find(\"p\").contents[0])\n",
    "print(\"value\", wb_dict[\"Value\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fila 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Página de prueba', 'Página de prueba']\n",
      "['main', nan]\n",
      "['p', 'a']\n",
      "['Este es el primer párrafo', 'https://pagina1.xyz/']\n"
     ]
    }
   ],
   "source": [
    "# Repetimos la acción para obtener `Title`\n",
    "title_content = soup.find('title')\n",
    "title_content_str = title_content.string\n",
    "wb_dict[\"Title\"].append(title_content_str)\n",
    "print(wb_dict[\"Title\"])\n",
    "\n",
    "# Vemos que https://pagina1.xyz/ no está en ningún `div` con `id` y por esto añadimos \n",
    "# np.nan o None como valor a la lista del diccionario `wb_dict`\n",
    "result = soup.find(\"div\", {\"class\":\"links\"})\n",
    "wb_dict[\"Id\"].append(np.nan)\n",
    "print(wb_dict[\"Id\"])\n",
    "\n",
    "# Buscamos el nombre del tag, en este caso de `a` y lo guardamos en el diccionario\n",
    "soup.find('a').name\n",
    "wb_dict[\"Tag\"].append(soup.find('a').name)\n",
    "print(wb_dict[\"Tag\"])\n",
    "\n",
    "# Busco el contenido que es el link y lo añado\n",
    "link1 = soup.find(\"a\")\n",
    "content = link1.get('href')\n",
    "wb_dict[\"Value\"].append(content)\n",
    "print(wb_dict[\"Value\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fila 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Página de prueba', 'Página de prueba', 'Página de prueba']\n",
      "['main', nan, nan]\n",
      "['p', 'a', 'a']\n",
      "['Este es el primer párrafo', 'https://pagina1.xyz/', 'https://pagina4.xyz/']\n"
     ]
    }
   ],
   "source": [
    "# Repetimos la acción para obtener `Title`\n",
    "title_content = soup.find('title')\n",
    "title_content_str = title_content.string\n",
    "wb_dict[\"Title\"].append(title_content_str)\n",
    "print(wb_dict[\"Title\"])\n",
    "\n",
    "# Vuelve a no haber `id` en el `div` y por eso añadimos np.nan \n",
    "result = soup.find(\"div\", {\"class\":\"links\"})\n",
    "wb_dict[\"Id\"].append(np.nan)\n",
    "print(wb_dict[\"Id\"])\n",
    "\n",
    "# Repetimos la acción de buscar el nombre\n",
    "soup.find('a').name\n",
    "wb_dict[\"Tag\"].append(soup.find('a').name)\n",
    "print(wb_dict[\"Tag\"])\n",
    "\n",
    "# Procuramos llegar hasta el enlace de esta fila, y una vez lo tenemos, lo añadimos al diccionario\n",
    "links = soup.findAll(\"a\")\n",
    "for pos, link in enumerate(links):\n",
    "    if pos == 3:\n",
    "        href = link.get('href')\n",
    "        wb_dict[\"Value\"].append(href)\n",
    "\n",
    "print(wb_dict[\"Value\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fila 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Página de prueba', 'Página de prueba', 'Página de prueba', 'Página de prueba']\n",
      "['main', nan, nan, nan]\n",
      "['p', 'a', 'a', 'a']\n",
      "['Este es el primer párrafo', 'https://pagina1.xyz/', 'https://pagina4.xyz/', 'https://pagina5.xyz/']\n"
     ]
    }
   ],
   "source": [
    "# Repetimos la acción para obtener `Title`\n",
    "title_content = soup.find('title')\n",
    "title_content_str = title_content.string\n",
    "wb_dict[\"Title\"].append(title_content_str)\n",
    "print(wb_dict[\"Title\"])\n",
    "\n",
    "# Vuelve a no haber `id` en el `div` y por eso añadimos np.nan \n",
    "result = soup.find(\"div\", {\"class\":\"links\"})\n",
    "wb_dict[\"Id\"].append(np.nan)\n",
    "print(wb_dict[\"Id\"])\n",
    "\n",
    "# Repetimos la acción de buscar el nombre\n",
    "soup.find('a').name\n",
    "wb_dict[\"Tag\"].append(soup.find('a').name)\n",
    "print(wb_dict[\"Tag\"])\n",
    "\n",
    "# Reutilizamos el código de la anterior fila para acceder al link y lo añadimos al diccionario\n",
    "links = soup.findAll(\"a\")\n",
    "for pos, link in enumerate(links):\n",
    "    if pos == 4:\n",
    "        href = link.get('href')\n",
    "        wb_dict[\"Value\"].append(href)\n",
    "\n",
    "print(wb_dict[\"Value\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fila 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Página de prueba', 'Página de prueba', 'Página de prueba', 'Página de prueba', 'Página de prueba']\n",
      "['main', nan, nan, nan, 'footer']\n",
      "['p', 'a', 'a', 'a', 'div']\n",
      "['Este es el primer párrafo', 'https://pagina1.xyz/', 'https://pagina4.xyz/', 'https://pagina5.xyz/', 'links footer-links']\n"
     ]
    }
   ],
   "source": [
    "# Repetimos la acción para obtener `Title`\n",
    "title_content = soup.find('title')\n",
    "title_content_str = title_content.string\n",
    "wb_dict[\"Title\"].append(title_content_str)\n",
    "print(wb_dict[\"Title\"])\n",
    "\n",
    "# El `id` de este `div` es footer  \n",
    "result = []\n",
    "for tag in soup.findAll(True,{'id':True}) :\n",
    "    result.append(tag['id'])\n",
    "\n",
    "wb_dict[\"Id\"].append(result[2])\n",
    "print(wb_dict[\"Id\"])\n",
    "\n",
    "# Repetimos la acción de buscar el nombre, en este caso de `div`\n",
    "soup.find(\"div\").name\n",
    "wb_dict[\"Tag\"].append(soup.find(\"div\").name)\n",
    "print(wb_dict[\"Tag\"])\n",
    "\n",
    "# En este caso queremos sacar 'links footer-links' como valor\n",
    "all_div =  soup.find(\"div\", id=\"footer\")\n",
    "all_div_1 = all_div.find(\"div\")\n",
    "result = all_div_1.get_attribute_list('class')\n",
    "final_result = [str(result[0])+ \" \" + str(result[1])]\n",
    "wb_dict[\"Value\"].append(final_result[0])\n",
    "print(wb_dict[\"Value\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fila 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Página de prueba', 'Página de prueba', 'Página de prueba', 'Página de prueba', 'Página de prueba', 'Página de prueba']\n",
      "['main', nan, nan, nan, 'footer', 'footer']\n",
      "['p', 'a', 'a', 'a', 'div', 'p']\n",
      "['Este es el primer párrafo', 'https://pagina1.xyz/', 'https://pagina4.xyz/', 'https://pagina5.xyz/', 'links footer-links', 'Este párrafo está en el footer']\n"
     ]
    }
   ],
   "source": [
    "# Repetimos la acción para obtener `Title`\n",
    "title_content = soup.find('title')\n",
    "title_content_str = title_content.string\n",
    "wb_dict[\"Title\"].append(title_content_str)\n",
    "print(wb_dict[\"Title\"])\n",
    "\n",
    "# El `id` de este `div` es footer \n",
    "result = []\n",
    "for tag in soup.findAll(True,{'id':True}) :\n",
    "    result.append(tag['id'])\n",
    "\n",
    "wb_dict[\"Id\"].append(result[2])\n",
    "print(wb_dict[\"Id\"])\n",
    "\n",
    "# Repetimos la acción de buscar el nombre\n",
    "soup.find(\"p\").name\n",
    "wb_dict[\"Tag\"].append(soup.find(\"p\").name)\n",
    "\n",
    "print(wb_dict[\"Tag\"])\n",
    "\n",
    "# Reutilizamos parte del código usado para esta misma columna en las filas 3 y 4\n",
    "content_p = soup.findAll(\"p\")\n",
    "for pos, elem in enumerate(content_p):\n",
    "    if pos == 2:\n",
    "        content_value = elem.contents\n",
    "        wb_dict[\"Value\"].append(content_value[0])\n",
    "        \n",
    "print(wb_dict[\"Value\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos el diccionario resultante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Title': ['Página de prueba', 'Página de prueba', 'Página de prueba', 'Página de prueba', 'Página de prueba', 'Página de prueba'], 'Id': ['main', nan, nan, nan, 'footer', 'footer'], 'Tag': ['p', 'a', 'a', 'a', 'div', 'p'], 'Value': ['Este es el primer párrafo', 'https://pagina1.xyz/', 'https://pagina4.xyz/', 'https://pagina5.xyz/', 'links footer-links', 'Este párrafo está en el footer']}\n"
     ]
    }
   ],
   "source": [
    "print(wb_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(wb_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Id</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Página de prueba</td>\n",
       "      <td>main</td>\n",
       "      <td>p</td>\n",
       "      <td>Este es el primer párrafo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Página de prueba</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a</td>\n",
       "      <td>https://pagina1.xyz/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Página de prueba</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a</td>\n",
       "      <td>https://pagina4.xyz/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Página de prueba</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a</td>\n",
       "      <td>https://pagina5.xyz/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Página de prueba</td>\n",
       "      <td>footer</td>\n",
       "      <td>div</td>\n",
       "      <td>links footer-links</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Página de prueba</td>\n",
       "      <td>footer</td>\n",
       "      <td>p</td>\n",
       "      <td>Este párrafo está en el footer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Title      Id  Tag                           Value\n",
       "0  Página de prueba    main    p       Este es el primer párrafo\n",
       "1  Página de prueba     NaN    a            https://pagina1.xyz/\n",
       "2  Página de prueba     NaN    a            https://pagina4.xyz/\n",
       "3  Página de prueba     NaN    a            https://pagina5.xyz/\n",
       "4  Página de prueba  footer  div              links footer-links\n",
       "5  Página de prueba  footer    p  Este párrafo está en el footer"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. From Amazon\n",
    "Using beautiful soap and/or regex\n",
    "\n",
    "Save in a dataframe the next information using web scraping. Using product pages from Amazon, do the following:\n",
    "\n",
    "Get the product name from the web and save it in a column called \"item_name\"\n",
    "Get the price from the web and save it in a column called \"item_price\"\n",
    "While you are doing the exercise, document the steps you are doing. Try to do the program for generic pages. If you cannot do it generic, explain the reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "url = https://www.amazon.es/Tommy-Hilfiger-UM0UM00054-Camiseta-Hombre/dp/B01MYD0T1F/ref=sr_1_1?dchild=1&pf_rd_p=58224bec-cac9-4dd2-a42a-61b1db609c2d&pf_rd_r=VZQ1JTQXFVRZ9E9VSKX4&qid=1595364419&s=apparel&sr=1-1\n",
    "\n",
    "item_name --> \"Tommy Hilfiger Logo Camiseta de Cuello Redondo,Perfecta para El Tiempo Libre para Hombre\"\n",
    "\n",
    "item_price --> [18,99 € - 46,59 €] or one of the options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "url = \"https://www.amazon.es/Tommy-Hilfiger-UM0UM00054-Camiseta-Hombre/dp/B01MYD0T1F/ref=sr_1_1?dchild=1&pf_rd_p=58224bec-cac9-4dd2-a42a-61b1db609c2d&pf_rd_r=VZQ1JTQXFVRZ9E9VSKX4&qid=1595364419&s=apparel&sr=1-1\"\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "soup1 = BeautifulSoup(response.content, features=\"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tommy Hilfiger Logo Camiseta de Cuello Redondo,Perfecta para El Tiempo Libre para Hombre\n"
     ]
    }
   ],
   "source": [
    "#Finding title from product and adding to item_name list.\n",
    "title = soup1.select(\"#productTitle\")[0].get_text().strip()\n",
    "print(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19,99 € - 34,95 €\n"
     ]
    }
   ],
   "source": [
    "#Finding price from product and adding to item_price list.\n",
    "price = soup1.select(\"#priceblock_ourprice\")[0].get_text()\n",
    "new_price = price.replace(u'\\xa0', u' ')\n",
    "print(new_price)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
